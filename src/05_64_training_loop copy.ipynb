{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's going to be a bit redundant for the sake of simplicity we're going to recreate the classes from previous notbooks here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_util\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albumentations Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT:\n",
    "# Resizing to 256√ó256 dramatically speeds up training\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.05,\n",
    "        scale_limit=0.10,\n",
    "        rotate_limit=15,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        p=0.5\n",
    "    ),\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polygon ‚Üí Mask (COCO conversion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_to_mask(anns, height, width):\n",
    "    \"\"\"\n",
    "    Convert COCO polygon annotations into a binary mask.\n",
    "    The dataset uses rectangular polygons, but the logic remains the same.\n",
    "    \"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for ann in anns:\n",
    "        rles = mask_util.frPyObjects(ann[\"segmentation\"], height, width)\n",
    "        rle = mask_util.merge(rles)\n",
    "        m = mask_util.decode(rle)\n",
    "        mask = np.maximum(mask, m)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dataset Class (TRAIN + VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads:\n",
    "      - Image: RGB\n",
    "      - Mask: binary mask generated from COCO polygons\n",
    "    Applies Albumentations transforms with correct (image,mask) pairing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        ann_path = os.path.join(root_dir, \"_annotations.coco.json\")\n",
    "        self.coco = COCO(ann_path)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Load COCO image metadata\n",
    "        # -----------------------------------------------------\n",
    "        img_id = self.image_ids[index]\n",
    "        info = self.coco.imgs[img_id]\n",
    "\n",
    "        file_name = info[\"file_name\"]\n",
    "        height = info[\"height\"]\n",
    "        width  = info[\"width\"]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.root_dir, file_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Load annotations ‚Üí polygon ‚Üí mask\n",
    "        # -----------------------------------------------------\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        mask = polygons_to_mask(anns, height, width).astype(np.uint8)\n",
    "        mask = np.expand_dims(mask, axis=2)   # HWC (Albumentations format)\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Albumentations transform (image, mask)\n",
    "        # -----------------------------------------------------\n",
    "        if self.transform:\n",
    "            data = self.transform(image=image, mask=mask)\n",
    "            image = data[\"image\"]           # CHW tensor\n",
    "            mask  = data[\"mask\"]            # HWC tensor\n",
    "\n",
    "            # FIX: Convert mask from HWC ‚Üí CHW\n",
    "            if mask.ndim == 3:\n",
    "                mask = mask.permute(2, 0, 1)\n",
    "\n",
    "        return image.float(), mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset (no masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads test images (no annotation file).\n",
    "    Returns:\n",
    "      - Image tensor\n",
    "      - Filename (used to save outputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = sorted([\n",
    "            f for f in os.listdir(root_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.filenames[idx]\n",
    "        path = os.path.join(self.root_dir, fn)\n",
    "\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        return image.float(), fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"MaxPool ‚Üí DoubleConv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsample ‚Üí Conv(1x1) ‚Üí concat skip ‚Üí DoubleConv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels // 2, in_channels // 2, kernel_size=1)\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.conv1x1(x1)\n",
    "\n",
    "        # Handle padding mismatch due to odd numbers\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(x1, [diffX//2, diffX-diffX//2,\n",
    "                        diffY//2, diffY-diffY//2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"Full U-Net\"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss + Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice = (2 * intersection + self.smooth) / \\\n",
    "               (probs.sum() + targets.sum() + self.smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    \"\"\"Recommended loss for medical segmentation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bce(logits, targets) + self.dice(logits, targets)\n",
    "\n",
    "\n",
    "def threshold_mask(logits, thr=0.5):\n",
    "    return (torch.sigmoid(logits) > thr).float()\n",
    "\n",
    "\n",
    "def compute_iou(logits, targets):\n",
    "    preds = threshold_mask(logits)\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (preds * targets).sum()\n",
    "    union = preds.sum() + targets.sum() - intersection\n",
    "    return ((intersection + 1e-6) / (union + 1e-6)).item()\n",
    "\n",
    "\n",
    "def compute_pixel_accuracy(logits, targets):\n",
    "    preds = threshold_mask(logits)\n",
    "    preds = preds.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    return (preds == targets).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Train samples: 1502\n",
      "Valid samples: 429\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BrainTumorDataset(\"../dataset/train\", transform=train_transform)\n",
    "val_dataset   = BrainTumorDataset(\"../dataset/valid\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Valid samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device + Model + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MPS is the GPU on Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "model = UNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = BCEDiceLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", patience=3, factor=0.5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training & Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, masks in tqdm(loader, desc=\"Training\"):\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # FP16 automatic casting for speed on MPS\n",
    "        with autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "\n",
    "        # NO GradScaler on MPS ‚Äî unnecessary\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    v_loss = v_iou = v_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Validating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "\n",
    "            v_loss += loss.item()\n",
    "            v_iou  += compute_iou(logits, masks)\n",
    "            v_acc  += compute_pixel_accuracy(logits, masks)\n",
    "\n",
    "    n = len(loader)\n",
    "    return v_loss/n, v_iou/n, v_acc/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_iou = 0\n",
    "# early_counter = 0\n",
    "\n",
    "# checkpoint_path = \"../experiments/best_model.pth\"\n",
    "\n",
    "# for epoch in range(1, 51):\n",
    "#     print(f\"\\n======== EPOCH {epoch} / 50 ========\")\n",
    "\n",
    "#     train_loss = train_one_epoch(model, train_loader)\n",
    "#     val_loss, val_iou, val_acc = validate_one_epoch(model, val_loader)\n",
    "\n",
    "#     print(f\" Train Loss: {train_loss:.4f}\")\n",
    "#     print(f\" Val Loss:   {val_loss:.4f}\")\n",
    "#     print(f\" Val IoU:    {val_iou:.4f}\")\n",
    "#     print(f\" Val Acc:    {val_acc:.4f}\")\n",
    "\n",
    "#     scheduler.step(val_iou)\n",
    "\n",
    "#     if val_iou > best_iou:\n",
    "#         print(f\"üî• IoU improved {best_iou:.4f} ‚Üí {val_iou:.4f}\")\n",
    "#         best_iou = val_iou\n",
    "#         early_counter = 0\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "#         print(\"üíæ Saved best model.\")\n",
    "#     else:\n",
    "#         early_counter += 1\n",
    "#         print(f\"No improvement: {early_counter} / 8\")\n",
    "\n",
    "#     if early_counter >= 8:\n",
    "#         print(\"‚õî Early stopping triggered.\")\n",
    "#         break\n",
    "\n",
    "# print(\"\\nTraining finished.\")\n",
    "# print(\"Best IoU achieved:\", best_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result we got so far is:\"0.6116\" likely bcs our dataset usees rectangular bounding-box polygons so this is effectively tumor detection, not organic tumor boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the model is:\n",
    "- Too shallow for 256√ó256\n",
    "- Possibly undertrained\n",
    "- Loss not optimized\n",
    "- Threshold too rigid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement A: High-Resolution Transforms (640√ó640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-resolution transforms ready (384x384).\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# IMPROVEMENT A:\n",
    "# Restore high-resolution images (640 √ó 640)\n",
    "# This boosts IoU significantly for rectangular masks.\n",
    "# ==========================================================\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "balanced_train_transform_safe = A.Compose([\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "balanced_val_transform_safe = A.Compose([\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"High-resolution transforms ready (384x384).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPROVEMENT B ‚Äî Wider U-Net (More Capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-capacity UNetWide model ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# IMPROVEMENT B:\n",
    "# Wider, deeper U-Net for higher IoU.\n",
    "# Doubles all channel counts.\n",
    "# ==========================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConvWide(nn.Module):\n",
    "    \"\"\"Two convolution blocks: Conv ‚Üí BN ‚Üí ReLU repeated\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DownWide(nn.Module):\n",
    "    \"\"\"MaxPool ‚Üí DoubleConvWide\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConvWide(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class UpWide(nn.Module):\n",
    "    \"\"\"Upsample ‚Üí 1x1 conv ‚Üí concat with skip ‚Üí DoubleConvWide\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels // 2, in_channels // 2, 1)\n",
    "        self.double_conv = DoubleConvWide(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.conv1x1(x1)\n",
    "\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(x1, [diffX//2, diffX - diffX//2,\n",
    "                        diffY//2, diffY - diffY//2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNetWide(nn.Module):\n",
    "    \"\"\"\n",
    "    High-capacity U-Net for improved IoU.\n",
    "    All layers doubled in width.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Balanced channels\n",
    "        self.inc = DoubleConvWide(n_channels, 96)\n",
    "        self.down1 = DownWide(96, 192)\n",
    "        self.down2 = DownWide(192, 384)\n",
    "        self.down3 = DownWide(384, 768)\n",
    "        self.down4 = DownWide(768, 768)\n",
    "\n",
    "        self.up1 = UpWide(1536, 384)\n",
    "        self.up2 = UpWide(768, 192)\n",
    "        self.up3 = UpWide(384, 96)\n",
    "        self.up4 = UpWide(192, 96)\n",
    "\n",
    "        self.out = nn.Conv2d(96, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return self.out(x)\n",
    "\n",
    "print(\"High-capacity UNetWide model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training with high-res + wide U-Net (A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Datasets for A+B loaded.\n",
      "Model, optimizer, scheduler for A+B ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Build NEW dataloaders & NEW model using improvements A + B\n",
    "# Without overwriting old variables.\n",
    "# ==========================================================\n",
    "\n",
    "# Dataset with new transforms\n",
    "train_dataset_D = BrainTumorDataset(\"../dataset/train\", transform=balanced_train_transform_safe)\n",
    "val_dataset_D   = BrainTumorDataset(\"../dataset/valid\", transform=balanced_val_transform_safe)\n",
    "\n",
    "train_loader_D = DataLoader(train_dataset_D, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader_D   = DataLoader(val_dataset_D, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "train_loader_AB = DataLoader(train_dataset_D, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader_AB   = DataLoader(val_dataset_D, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Datasets for A+B loaded.\")\n",
    "\n",
    "# Model for A+B\n",
    "model_AB = UNetWide().to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_AB = optim.Adam(model_AB.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss (same)\n",
    "criterion_AB = BCEDiceLoss()\n",
    "\n",
    "# LR scheduler\n",
    "scheduler_AB = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_AB,\n",
    "    mode=\"max\",\n",
    "    patience=4,\n",
    "    factor=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model, optimizer, scheduler for A+B ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Improved A+B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================================\n",
    "# # Train improved UNetWide on 640√ó640 images (A+B)\n",
    "# # ==========================================================\n",
    "\n",
    "# best_iou_AB = 0\n",
    "# early_counter_AB = 0\n",
    "# MAX_EPOCHS_AB = 40\n",
    "# PATIENCE_AB = 10\n",
    "\n",
    "# checkpoint_AB = \"../experiments/best_model_AB.pth\"\n",
    "\n",
    "# for epoch in range(1, MAX_EPOCHS_AB + 1):\n",
    "#     print(f\"\\n======== [A+B] EPOCH {epoch}/{MAX_EPOCHS_AB} ========\")\n",
    "\n",
    "#     train_loss = train_one_epoch(model_AB, train_loader_AB)\n",
    "#     val_loss, val_iou, val_acc = validate_one_epoch(model_AB, val_loader_AB)\n",
    "\n",
    "#     print(f\" Train Loss: {train_loss:.4f}\")\n",
    "#     print(f\" Val Loss:   {val_loss:.4f}\")\n",
    "#     print(f\" Val IoU:    {val_iou:.4f}\")\n",
    "#     print(f\" Val Acc:    {val_acc:.4f}\")\n",
    "\n",
    "#     scheduler_AB.step(val_iou)\n",
    "\n",
    "#     if val_iou > best_iou_AB:\n",
    "#         print(f\"üî• IoU improved {best_iou_AB:.4f} ‚Üí {val_iou:.4f}\")\n",
    "#         best_iou_AB = val_iou\n",
    "#         early_counter_AB = 0\n",
    "#         torch.save(model_AB.state_dict(), checkpoint_AB)\n",
    "#         print(\"üíæ Saved best A+B model.\")\n",
    "#     else:\n",
    "#         early_counter_AB += 1\n",
    "#         print(f\"No improvement: {early_counter_AB}/{PATIENCE_AB}\")\n",
    "\n",
    "#     if early_counter_AB >= PATIENCE_AB:\n",
    "#         print(\"‚õî Early stopping triggered.\")\n",
    "#         break\n",
    "\n",
    "# print(\"Training (A+B) finished.\")\n",
    "# print(\"Best IoU achieved (A+B):\", best_iou_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# CORRECT STANDARD U-NET FOR APPLE SILICON (MPS SAFE)\n",
    "# ==========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv ‚Üí BN ‚Üí ReLU) √ó 2\"\"\"\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscale ‚Üí DoubleConv\"\"\"\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_c, out_c)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsample ‚Üí Concatenate ‚Üí DoubleConv\"\"\"\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_c, out_c)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # match dimensions\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"Correct standard U-Net architecture\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # dropout in bottleneck (optional but improves generalization)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.up1 = Up(1024 + 512, 512)\n",
    "        self.up2 = Up(512 + 256, 256)\n",
    "        self.up3 = Up(256 + 128, 128)\n",
    "        self.up4 = Up(128 + 64, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x5 = self.dropout(x5)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return self.out(x)\n",
    "    \n",
    "def validate_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_iou += compute_iou(logits, masks)\n",
    "            running_acc += compute_pixel_accuracy(logits, masks)\n",
    "\n",
    "    return (\n",
    "        running_loss / len(loader),\n",
    "        running_iou / len(loader),\n",
    "        running_acc / len(loader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# UNet128-STABLE ‚Äî channel-correct decoder\n",
    "# ==========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConvR(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DownR(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConvR(in_c, out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.mp(x))\n",
    "\n",
    "\n",
    "class UpR(nn.Module):\n",
    "    \"\"\"\n",
    "    Explicitly specify skip channel sizes to avoid mismatches.\n",
    "    \"\"\"\n",
    "    def __init__(self, up_in_c, skip_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = DoubleConvR(up_in_c + skip_c, out_c)\n",
    "\n",
    "    def forward(self, x_up, x_skip):\n",
    "        x_up = self.up(x_up)\n",
    "\n",
    "        diffY = x_skip.size(2) - x_up.size(2)\n",
    "        diffX = x_skip.size(3) - x_up.size(3)\n",
    "\n",
    "        x_up = F.pad(x_up,\n",
    "                     [diffX//2, diffX - diffX//2,\n",
    "                      diffY//2, diffY - diffY//2])\n",
    "\n",
    "        x = torch.cat([x_skip, x_up], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetStable128(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully stable UNet with correct decoder math.\n",
    "    Very strong on 128√ó128 tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ENCODER\n",
    "        self.inc   = DoubleConvR(n_channels, 96)\n",
    "        self.down1 = DownR(96,   192)\n",
    "        self.down2 = DownR(192,  384)\n",
    "        self.down3 = DownR(384,  768)\n",
    "        self.down4 = DownR(768, 1024)\n",
    "\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "\n",
    "        # DECODER with explicit correct channel sizes\n",
    "        self.up1 = UpR(1024, 768, 384)  # up x5 + x4\n",
    "        self.up2 = UpR(384,  384, 192)  # up + x3\n",
    "        self.up3 = UpR(192,  192, 96)   # up + x2\n",
    "        self.up4 = UpR(96,   96,  64)   # up + x1\n",
    "\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)      # 96\n",
    "        x2 = self.down1(x1)   # 192\n",
    "        x3 = self.down2(x2)   # 384\n",
    "        x4 = self.down3(x3)   # 768\n",
    "        x5 = self.down4(x4)   # 1024\n",
    "\n",
    "        x5 = self.drop(x5)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x,  x3)\n",
    "        x = self.up3(x,  x2)\n",
    "        x = self.up4(x,  x1)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# STRICT NON-GEOMETRIC TRANSFORMS (SAFE FOR BBOX MASKS)\n",
    "# ==========================================================\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(64, 64),     # ‚Üê REDUCED RESOLUTION\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(64, 64),     # ‚Üê REDUCED RESOLUTION\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BrainTumorDataset(\"../dataset/train\", transform=train_transform)\n",
    "val_dataset   = BrainTumorDataset(\"../dataset/valid\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: mps\n",
      "\n",
      "üöÄ Starting Training...\n",
      "\n",
      "\n",
      "========== EPOCH 1/30 ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|‚ñè         | 6/376 [00:07<08:08,  1.32s/it, loss=1.52]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     76\u001b[39m masks = masks.to(DEVICE)\n\u001b[32m     78\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m loss = criterion(logits, masks)\n\u001b[32m     83\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mUNetStable128.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     92\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up2(x,  x3)\n\u001b[32m     93\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up3(x,  x2)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mUpR.forward\u001b[39m\u001b[34m(self, x_up, x_skip)\u001b[39m\n\u001b[32m     49\u001b[39m x_up = F.pad(x_up,\n\u001b[32m     50\u001b[39m              [diffX//\u001b[32m2\u001b[39m, diffX - diffX//\u001b[32m2\u001b[39m,\n\u001b[32m     51\u001b[39m               diffY//\u001b[32m2\u001b[39m, diffY - diffY//\u001b[32m2\u001b[39m])\n\u001b[32m     53\u001b[39m x = torch.cat([x_skip, x_up], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mDoubleConvR.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# TRAINING LOOP (MPS SAFE) + PROGRESS BAR + VISUAL PREVIEW\n",
    "# ==========================================================\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using:\", DEVICE)\n",
    "\n",
    "model = UNetStable128().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = BCEDiceLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", patience=3, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "best_iou = 0\n",
    "patience = 8\n",
    "counter = 0\n",
    "max_epochs = 30\n",
    "\n",
    "\n",
    "def show_visual_progress(model, loader):\n",
    "    \"\"\"Show overlay of image + ground truth + prediction.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img, mask = next(iter(loader))\n",
    "        img = img.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "\n",
    "        pred = torch.sigmoid(model(img))\n",
    "        pred_bin = (pred > 0.5).float()\n",
    "\n",
    "        # Convert to numpy\n",
    "        img_np = img[0].permute(1,2,0).cpu().numpy()\n",
    "        img_np = img_np * 0.5 + 0.5  # unnormalize\n",
    "        mask_np = mask[0,0].cpu().numpy()\n",
    "        pred_np = pred_bin[0,0].cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(16,5))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title(\"Image\")\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.imshow(img_np, alpha=0.7)\n",
    "        plt.imshow(mask_np, cmap=\"Reds\", alpha=0.4)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.imshow(img_np, alpha=0.7)\n",
    "        plt.imshow(pred_np, cmap=\"Reds\", alpha=0.4)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nüöÄ Starting Training...\\n\")\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    print(f\"\\n========== EPOCH {epoch}/{max_epochs} ==========\\n\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=\"Training\", leave=True)\n",
    "\n",
    "    for images, masks in loop:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, masks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # VALIDATION\n",
    "    val_loss, val_iou, val_acc = validate_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"Val IoU:    {val_iou:.4f}\")\n",
    "    print(f\"Val Acc:    {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(val_iou)\n",
    "\n",
    "    # --- EARLY STOPPING ---\n",
    "    if val_iou > best_iou:\n",
    "        print(f\"üåü Best IoU improved {best_iou:.4f} ‚Üí {val_iou:.4f}\")\n",
    "        best_iou = val_iou\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"../experiments/best_model_mps.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"‚õî Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # --- VISUAL PROGRESS EVERY 2 EPOCHS ---\n",
    "    if epoch % 2 == 0:\n",
    "        print(\"\\nüìä Visual Progress Preview:\")\n",
    "        show_visual_progress(model, val_loader)\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(\"Best IoU achieved:\", best_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
